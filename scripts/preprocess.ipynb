{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nJust run all of these to completion to preprocess the data and load it into the raw folder\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Just run all of these to completion to preprocess the data and load it into the raw folder\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/08/17 13:53:12 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/08/17 13:53:12 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Create a Spark Session\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow.parquet as pq\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Assignment\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+-------------+------------+------------+-----------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|trip_distance|PULocationID|DOLocationID|fare_amount|total_amount|\n",
      "+--------------------+---------------------+-------------+------------+------------+-----------+------------+\n",
      "| 2019-02-01 00:59:04|  2019-02-01 01:07:27|          2.1|          48|         234|        9.0|        12.3|\n",
      "| 2019-02-01 00:33:09|  2019-02-01 01:03:58|          9.8|         230|          93|       32.0|        33.3|\n",
      "| 2019-02-01 00:09:03|  2019-02-01 00:09:16|          0.0|         145|         145|        2.5|         3.8|\n",
      "| 2019-02-01 00:45:38|  2019-02-01 00:51:10|          0.8|          95|          95|        5.5|         6.8|\n",
      "| 2019-02-01 00:25:30|  2019-02-01 00:28:14|          0.8|         140|         263|        5.0|         6.3|\n",
      "| 2019-02-01 00:38:02|  2019-02-01 00:40:57|          0.8|         229|         141|        4.5|         5.8|\n",
      "| 2019-02-01 00:06:49|  2019-02-01 00:10:34|          0.9|          75|          41|        5.0|         6.3|\n",
      "| 2019-02-01 00:04:04|  2019-02-01 00:24:27|          2.8|         246|         229|       14.0|        15.3|\n",
      "| 2019-02-01 00:28:20|  2019-02-01 00:40:31|          2.1|          79|         232|       10.5|        11.8|\n",
      "| 2019-01-31 23:16:28|  2019-01-31 23:19:11|         0.49|         170|         234|        4.0|         7.0|\n",
      "| 2019-01-31 23:28:02|  2019-01-31 23:36:34|         1.61|         107|         161|        8.0|       11.62|\n",
      "| 2019-01-31 23:55:34|  2019-02-01 00:03:58|          1.5|         107|         148|        7.5|       10.56|\n",
      "| 2019-02-01 00:15:53|  2019-02-01 00:25:41|          1.8|          68|         246|        9.0|       12.35|\n",
      "| 2019-02-01 00:28:54|  2019-02-01 00:33:51|          0.8|         246|          68|        5.5|         8.8|\n",
      "| 2019-02-01 00:21:20|  2019-02-01 00:25:13|          0.4|         161|         162|        4.5|         5.8|\n",
      "| 2019-01-31 23:58:23|  2019-02-01 00:22:45|         4.43|         161|         249|       18.5|       24.75|\n",
      "| 2019-02-01 00:43:11|  2019-02-01 00:48:33|          1.0|         144|         232|        6.0|         7.3|\n",
      "| 2019-02-01 00:15:39|  2019-02-01 00:32:28|          9.0|         185|         166|       26.0|        34.8|\n",
      "| 2019-02-01 00:08:17|  2019-02-01 00:16:56|          1.8|         163|         239|        8.5|       11.75|\n",
      "| 2019-02-01 00:29:15|  2019-02-01 00:51:04|         10.3|         161|          18|       30.0|        34.3|\n",
      "+--------------------+---------------------+-------------+------------+------------+-----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Data preprocessing for yellow taxi tripdata 2019/02-07 parquet files \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col, month, hour, to_date, dayofweek\n",
    "\n",
    "# data cleansing for yellow taxi \n",
    "yellow_feb = spark.read.parquet(\"data/landing/yellow_tripdata_2019-02.parquet\")\n",
    "yellow_march = spark.read.parquet(\"data/landing/yellow_tripdata_2019-03.parquet\")\n",
    "yellow_april = spark.read.parquet(\"data/landing/yellow_tripdata_2019-04.parquet\")\n",
    "yellow_may = spark.read.parquet(\"data/landing/yellow_tripdata_2019-05.parquet\")\n",
    "yellow_june= spark.read.parquet(\"data/landing/yellow_tripdata_2019-06.parquet\")\n",
    "yellow_july = spark.read.parquet(\"data/landing/yellow_tripdata_2019-07.parquet\")\n",
    "\n",
    "# merge the yellow taxi data\n",
    "yellow_taxi_all = yellow_feb.unionAll(yellow_march).unionAll(yellow_april).unionAll(yellow_may).unionAll(yellow_june).unionAll(yellow_july)\n",
    "\n",
    "# remove irrelevant columns\n",
    "irr_columns = ['VendorID', 'passenger_count', 'store_and_fwd_flag', 'RatecodeID', 'payment_type', 'extra', 'mta_tax', \n",
    "                'tip_amount', 'tolls_amount', 'improvement_surcharge', 'congestion_surcharge', 'airport_fee']\n",
    "yellow_taxi_all = yellow_taxi_all.drop(*irr_columns)\n",
    "\n",
    "# remove null values for columns that I need to exclude null values \n",
    "yellow_drop_na_cols = ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'trip_distance', \n",
    "                'PULocationID', 'DOLocationID', 'fare_amount']\n",
    "\n",
    "yellow_taxi_all = yellow_taxi_all.dropna(subset=yellow_drop_na_cols)\n",
    "yellow_taxi_all.show()\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "yellow_taxi_all.repartition(1).write.format(\"parquet\").mode(\"append\").save(\"data/raw/raw_yellow_taxi\")\n",
    "\n",
    "# NOTE - as the data saves as a folder as opposed to a file, you will have to rename the parquet file in the output folder\n",
    "# please rename the generated parquet file to raw_yellow_taxi.parquet and place in in the raw folder and also remove the generated folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "External data pre-processing event data: https://data.cityofnewyork.us/Recreation/Parks-Special-Events/6v4b-5gp4/data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pyspark.sql.functions import lit\n",
    "from pathlib import Path  \n",
    "\n",
    "# event data pre-processing\n",
    "event = pd.read_csv(\"data/landing/Parks_Special_Events.csv\")\n",
    "\n",
    "# remove irrelevant columns \n",
    "event = event.drop(columns = [\"Group Name/Partner\", \"Unit\", \"LocationType\", \"Event Name\", \"Event Type\", \"Classification\", \"Attendance\",\n",
    "                                                \"Location\", \"Category\", \"Audience\"])\n",
    "\n",
    "# parse one column in two columns (date and time)\n",
    "date_format = \"%m/%d/%Y %I:%M:%S %p\"\n",
    "event[\"Date and Time\"] = pd.to_datetime(event[\"Date and Time\"], format= date_format)\n",
    "event.rename(columns = { \"Date and Time\" :\"date\"}, inplace = True)\n",
    "event['hour'] = event[\"date\"].dt.hour\n",
    "event['date'] = event[\"date\"].dt.date\n",
    "\n",
    "event = event.rename(columns={\"date\": \"date2\", \"Borough\": \"Borough2\", \"hour\": \"hour2\"})\n",
    "\n",
    "filepath = Path('data/raw/raw_events.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "event.to_csv(filepath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PULocationID        Borough\n",
      "0               1            EWR\n",
      "1               2         Queens\n",
      "2               3          Bronx\n",
      "3               4      Manhattan\n",
      "4               5  Staten Island\n",
      "..            ...            ...\n",
      "258           259          Bronx\n",
      "259           260         Queens\n",
      "260           261      Manhattan\n",
      "261           262      Manhattan\n",
      "262           263      Manhattan\n",
      "\n",
      "[263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "External data pre-processing for taxi zone lookup data\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path  \n",
    "\n",
    "# event data pre-processing\n",
    "taxi_zone = pd.read_csv(\"data/landing/taxi+_zone_lookup.csv\").drop(columns=[\"Zone\", \"service_zone\"])\n",
    "taxi_zone = taxi_zone.rename(columns={\"LocationID\": \"PULocationID\"})\n",
    "\n",
    "taxi_zone = taxi_zone.query(\"Borough != 'Unknown'\")\n",
    "\n",
    "print(taxi_zone)\n",
    "\n",
    "filepath = Path('data/raw/raw_taxi_zones.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "taxi_zone.to_csv(filepath)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date temp_categorized rained snowed\n",
      "0   2019-02-01             Cold     No     No\n",
      "1   2019-02-02             Cold     No     No\n",
      "2   2019-02-03             Cold     No     No\n",
      "3   2019-02-04             Cold     No     No\n",
      "4   2019-02-05         Moderate     No     No\n",
      "..         ...              ...    ...    ...\n",
      "176 2019-07-27              Hot     No     No\n",
      "177 2019-07-28              Hot    Yes     No\n",
      "178 2019-07-29              Hot     No     No\n",
      "179 2019-07-30              Hot    Yes     No\n",
      "180 2019-07-31              Hot    Yes     No\n",
      "\n",
      "[181 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "External data pre-processing for weather data at https://www.kaggle.com/datasets/alejopaullier/new-york-city-weather-data-2019 \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from pathlib import Path  \n",
    "\n",
    "import pandas as pd \n",
    "weather = pd.read_csv(\"data/landing/nyc_temperature.csv\")\n",
    "\n",
    "# parse data by row (01/02/19 - 31/07/2019)\n",
    "weather = weather.iloc[31:211 + 1]\n",
    "weather = weather.reset_index(drop=True)\n",
    "\n",
    "# checked there's no snow from feb to july \n",
    "# delete snow relevant columns \n",
    "del_col = ['tmax','tmin','departure','HDD','CDD','snow_depth']\n",
    "for column in del_col:\n",
    "    del weather[column]\n",
    "\n",
    "# check if rained/snowed or not \n",
    "weather['rained'] = weather['precipitation'] > '0'\n",
    "weather['rained'] = weather['rained'].map({True: 'Yes', False: 'No'})\n",
    "\n",
    "# check if snowed or not\n",
    "weather['snowed'] = weather['new_snow'] > '0'\n",
    "weather['snowed'] = weather['snowed'].map({True: 'Yes', False: 'No'})\n",
    "\n",
    "del_col2 = ['precipitation', 'new_snow']\n",
    "for column in del_col2:\n",
    "    del weather[column]\n",
    "\n",
    "# categorize average temperatures in 3 categories; cold, moderate, hot\n",
    "temp_categorize = {\n",
    "    'Cold': (-float('inf'), 50),\n",
    "    'Moderate': (50, 77),\n",
    "    'Hot': (77, float('inf'))\n",
    "}\n",
    "\n",
    "def categorize_temp(temp):\n",
    "    for category, (lower, upper) in temp_categorize.items():\n",
    "        if lower <= temp < upper:\n",
    "            return category\n",
    "\n",
    "weather['tavg'] = weather['tavg'].apply(categorize_temp)\n",
    "weather.rename(columns={'tavg': 'temp_categorized'}, inplace=True)\n",
    "\n",
    "# convert weather date to timestamp\n",
    "weather['date'] = pd.to_datetime(weather['date'], format='%d/%m/%y')\n",
    "\n",
    "print(weather)\n",
    "\n",
    "filepath = Path('data/raw/raw_weather.csv')  \n",
    "filepath.parent.mkdir(parents=True, exist_ok=True)  \n",
    "weather.to_csv(filepath)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

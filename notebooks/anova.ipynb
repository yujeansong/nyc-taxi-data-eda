{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Create a Spark Session\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyarrow.parquet as pq\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Assignment\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+----+-----------+----------------+------+------+---------+-----+\n",
      "|      date|total_amount|hour|day of week|temp_categorized|rained|snowed|  Borough|event|\n",
      "+----------+------------+----+-----------+----------------+------+------+---------+-----+\n",
      "|2019-05-08|        14.0|   0|          4|        Moderate|    No|    No|Manhattan|false|\n",
      "|2019-05-08|         8.3|   0|          4|        Moderate|    No|    No|Manhattan|false|\n",
      "|2019-05-08|       14.76|   0|          4|        Moderate|    No|    No|Manhattan|false|\n",
      "|2019-05-27|       16.56|  14|          2|        Moderate|    No|    No|Manhattan|false|\n",
      "|2019-02-23|         9.8|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|       23.14|  18|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|        15.3|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|       15.38|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|         6.8|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|         5.3|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|       73.27|   0|          7|            Cold|   Yes|    No|   Queens|false|\n",
      "|2019-02-23|        10.8|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|       12.96|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|        55.3|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|       18.36|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|        11.3|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|        8.97|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|        17.3|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|       13.56|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "|2019-02-23|        12.3|   0|          7|            Cold|   Yes|    No|Manhattan|false|\n",
      "+----------+------------+----+-----------+----------------+------+------+---------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Open the parquet file we just saved\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from pyspark.sql.functions import col, month, hour, to_date, dayofweek\n",
    "\n",
    "# data cleansing for yellow taxi \n",
    "data = spark.read.parquet(\"data/curated/curated_yellow_taxi.parquet\")\n",
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===========================================>              (3 + 1) / 4]\r"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Perform all anova analysis\n",
    "\n",
    "\"\"\"\n",
    "from scipy.stats import f_oneway\n",
    "import numpy as np\n",
    "\n",
    "# temperature\n",
    "hourly_temp_data = data.groupBy('date', 'hour', 'temp_categorized').count().drop('date', 'hour').toPandas()\n",
    "temp_anova_event = f_oneway(hourly_temp_data[hourly_temp_data['temp_categorized'] == 'Cold']['count'],\n",
    "                      hourly_temp_data[hourly_temp_data['temp_categorized'] == 'Moderate']['count'],\n",
    "                      hourly_temp_data[hourly_temp_data['temp_categorized'] == 'Hot']['count'])\n",
    "\n",
    "print(temp_anova_event)\n",
    "# F_onewayResult(statistic=23.320703203270472, pvalue=8.433015701890361e-11)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_onewayResult(statistic=20.88765002215742, pvalue=2.8470674250821966e-24)\n",
      "4343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# day of week\n",
    "\n",
    "hourly_day_of_week_data = data.groupBy('date', 'hour', 'day of week').count().drop('date', 'hour').toPandas()\n",
    "\n",
    "hourly_day_of_week_anova_event = f_oneway(hourly_day_of_week_data[hourly_day_of_week_data['day of week'] == 1]['count'],\n",
    "                      hourly_day_of_week_data[hourly_day_of_week_data['day of week'] == 2]['count'],\n",
    "                      hourly_day_of_week_data[hourly_day_of_week_data['day of week'] == 3]['count'],\n",
    "                      hourly_day_of_week_data[hourly_day_of_week_data['day of week'] == 4]['count'],\n",
    "                      hourly_day_of_week_data[hourly_day_of_week_data['day of week'] == 5]['count'],\n",
    "                      hourly_day_of_week_data[hourly_day_of_week_data['day of week'] == 6]['count'],\n",
    "                      hourly_day_of_week_data[hourly_day_of_week_data['day of week'] == 7]['count'])\n",
    "\n",
    "print(hourly_day_of_week_anova_event)\n",
    "# F_onewayResult(statistic=20.88765002215742, pvalue=2.8470674250821966e-24)\n",
    "\n",
    "\n",
    "print(len(hourly_day_of_week_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_onewayResult(statistic=398.1111490509572, pvalue=0.0)\n",
      "24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# time of day\n",
    "hourly_time_of_day_data = data.groupBy('date', 'hour').count().drop('date').toPandas()\n",
    "times = [x for x in range(0,24)]\n",
    "\n",
    "hourly_time_of_day_data = [hourly_time_of_day_data[hourly_time_of_day_data['hour'] == h]['count'] for h in times]\n",
    "hourly_time_of_day_anova_event = f_oneway(*hourly_time_of_day_data)\n",
    "\n",
    "print(hourly_time_of_day_anova_event)\n",
    "# extremely small p-values rounds to zero, but since f-stats is sufficiently large it's not error\n",
    "F_onewayResult(statistic=398.1111490509572, pvalue=0.0)\n",
    "print(len(hourly_time_of_day_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F_onewayResult(statistic=9506.069945822808, pvalue=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Borough\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "distinct_boroughs = data.select('borough').distinct()\n",
    "# distinct_boroughs.show()\n",
    "boroughs = [\"Queens\", \"EWR\", \"Brooklyn\", \"Staten Island\", \"Manhattan\", \"Bronx\"]\n",
    "\n",
    "hourly_borough_data = data.groupBy('date', 'hour', 'borough').count().drop('date', 'hour').toPandas()\n",
    "\n",
    "hourly_borough_data = [hourly_borough_data[hourly_borough_data['borough'] == b]['count'] for b in boroughs]\n",
    "hourly_borough_anova_event = f_oneway(*hourly_borough_data)\n",
    "\n",
    "print(hourly_borough_anova_event)\n",
    "# extremely small p-values rounds to zero, but since f-stats is sufficiently large it's not error\n",
    "# F_onewayResult(statistic=9506.069945822808, pvalue=0.0)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
